{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3802c55-796a-4f88-afc2-1ba80c0b055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "\n",
    "#model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b29713-c382-42ee-849e-fed7386d0710",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a996d0-7751-4c2e-911d-15c8b139d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'paragraphs'])\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "f = open(\"../wikipedia.json\", \"r\")\n",
    "data = json.load(f)\n",
    "\n",
    "articles = [art for art in data['data']]\n",
    "print(articles[0].keys())\n",
    "articles = [{\"title\": a['title'], \"content\": \"\\n\".join([p['context'] for p in a['paragraphs']])} for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fa5d39-aa22-4e94-83aa-596e66630b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "docs = [Document(page_content=a['content'], metadata={\"title\":a['title']}) for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c575c051-c717-4b6b-9ab1-bf30e574319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de68ea9-656c-4c49-8f51-d118f9d17745",
   "metadata": {},
   "source": [
    "## Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c86a295-402e-4e09-af7c-feded5736610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df607ba7597c4e8e8d001d28f731c250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            #quantization_config=bnb_config\n",
    "        ).to('cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer, \n",
    "                device=0,\n",
    "                max_new_tokens=1024)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814efbe5-9ed5-40ac-99cd-355f52b169cf",
   "metadata": {},
   "source": [
    "## Create RAGAS dataset from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be583f5d-d510-4754-a44e-5c1ca4a7db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/home/u024236/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/cfs/home/u024236/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/cfs/home/u024236/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "/cfs/home/u024236/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/ragas/metrics/__init__.py:8: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n",
      "/cfs/home/u024236/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestsetGenerator(generator_llm=LangchainLLMWrapper(run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=(<class 'Exception'>,), log_tenacity=False, seed=42)), critic_llm=LangchainLLMWrapper(run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=(<class 'Exception'>,), log_tenacity=False, seed=42)), embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x7ff0788e8690>, docstore=InMemoryDocumentStore(splitter=<langchain_text_splitters.base.TokenTextSplitter object at 0x7ff0788e80d0>, nodes=[], node_embeddings_list=[], node_map={}, run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=(<class 'Exception'>,), log_tenacity=False, seed=42)))\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# documents = load your documents\n",
    "\n",
    "# generator with openai models\n",
    "#generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "#critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "generator_llm = llm\n",
    "critic_llm = llm\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Change resulting question type distribution\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "print(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa80500-211c-407a-9e19-12a469564931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use generator.generate_with_llamaindex_docs if you use llama-index as document loader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/ragas/testset/generator.py:206\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    204\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/ragas/testset/docstore.py:214\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[0;34m(self, docs, show_progress)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[1;32m    210\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    211\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[1;32m    213\u001b[0m ]\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Phi3.5/env_phi35/lib64/python3.11/site-packages/ragas/testset/docstore.py:253\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    251\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nodes_to_embed\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "# use generator.generate_with_llamaindex_docs if you use llama-index as document loader\n",
    "testset = generator.generate_with_langchain_docs(documents, 2, distributions,raise_exceptions=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552032d-0bb4-4fd9-8c72-920ff848f856",
   "metadata": {},
   "source": [
    "Problem is in docstore the nodes appear to come with Embedding and Keyphrases but empty so they dont go into the executor to get embeddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afae28-fcf5-4b65-ac43-eceb9fdc9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d0414-b9fd-45c2-8239-27dc1d1a79e5",
   "metadata": {},
   "source": [
    "## Try with Langchain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e224690-f1e8-4dcb-9ffb-c55692075e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "documents = WikipediaLoader(query=\"Intelligence\", load_max_docs=10).load()\n",
    "len(documents)\n",
    "\n",
    "# use generator.generate_with_llamaindex_docs if you use llama-index as document loader\n",
    "testset = generator.generate_with_langchain_docs(documents, 2, distributions,raise_exceptions=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941efabe-943c-40cf-82ca-b1c7e5eb795e",
   "metadata": {},
   "source": [
    "## Try Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89d674-8f1b-449d-ae0b-f71f34f4bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_index.core import Document\n",
    "\n",
    "# Read the JSON file\n",
    "f = open(\"../wikipedia.json\", \"r\")\n",
    "data = json.load(f)\n",
    "articles = [art for art in data['data']]\n",
    "\n",
    "# Print the keys of the first article (for debugging)\n",
    "print(articles[0].keys())\n",
    "\n",
    "# Process the articles\n",
    "articles = [{\"title\": a['title'], \"content\": \"\\n\".join([p['context'] for p in a['paragraphs']])} for a in articles]\n",
    "\n",
    "# Create LlamaIndex Document objects\n",
    "docs = [Document(text=a['content'], metadata={\"title\": a['title']}) for a in articles]\n",
    "\n",
    "# use generator.generate_with_llamaindex_docs if you use llama-index as document loader\n",
    "testset = generator.generate_with_langchain_docs(docs, 2, distributions,raise_exceptions=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d30055-2a8c-4028-b6da-1d12e6340a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_phi35",
   "language": "python",
   "name": "env_phi35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
